<h1 align="center">Languageâ€‘Assistedâ€¯RLâ€¯Agent</h1>

<p align="center">
  <a href="LICENSE"><img src="https://img.shields.io/badge/License-MIT-green.svg"/></a>
  <img src="https://img.shields.io/badge/Python-3.11-blue.svg"/>
  <img src="https://img.shields.io/badge/PPO-clipâ€‘styleâ€‘update-orange"/>
</p>

---

## ğŸš€ Overview
This project bridges **Natural Language Understanding** and **Reinforcement Learning** by teaching a PPOâ€‘based agent to execute freeâ€‘form text commands inside a grid world.  
Our contributions:

* **Dualâ€‘stream PPO** that consumes spatial & textual cues.  
* **Crossâ€‘attention** over visual features + language embeddings.  
* **BERT fineâ€‘tuning** (contrastive + sequenceâ€‘classification) to disambiguate subtly different instructions.  
* Endâ€‘toâ€‘end code for data generation, training, and evaluation.

<p align="center">
  <img src="assets/images/env.png" alt="Environment overview" width="35%">
</p>

---

## ğŸ“‘ Table of Contents
1. [Environment](#environment)
2. [Dataset](#dataset)
3. [Theoretical Background](#theoretical-background)
4. [Methodology](#methodology)
5. [ExperimentsÂ &Â Results](#experiments--results)
6. [Quick Start](#quick-start)
7. [Report & Poster](#report--poster)
8. [References](#references)

---

## Environment
A **10â€¯Ã—â€¯10** grid; the goal is always one of the four corners.  
The agentâ€™s action space: **â†‘â€¯â†“â€¯â†â€¯â†’â€¯âœ”ï¸**.  
Reward =Â âˆ’1/step, +60 on correct â€œconfirmâ€. Optimal returnÂ =Â 51.

---

## Dataset
We generated language prompts with the **GeminiÂ API**, each mapped to its corner goal.

| Example Prompt                                                  | Goal |
|-----------------------------------------------------------------|------|
| Navigate to the terminal cell, utmost on the rightmost borderÂ Â  | (0,â€¯9) |
| Proceed to the far left of the bottom rowÂ                       | (9,â€¯0) |

---

## Theoretical Background
We base our agent on **Proximal Policy Optimization** (PPO).  
The clipped surrogate objective is  

$$
L_\text{clip}(Î¸)=\hat{\mathbb{E}}_t\Big[\min\big(r_t(Î¸)\hat{A}_t,\;\mathrm{clip}(r_t(Î¸),1-\epsilon,1+\epsilon)\hat{A}_t\big)\Big],
$$

with $r_t=\frac{Ï€_Î¸}{Ï€_{Î¸_\text{old}}}$.  
<p align="center">
  <img src="assets/images/ppo_overview.png" alt="PPO overview" width="40%">
</p>

---

## Methodology
### 1. Multiâ€‘Input PPO  
* **State** = `(x,â€¯y)` **+** BERT text embedding  
* Actor & critic each twoâ€‘tower MLP â†’ concatenation â†’ dense head

### 2. Visual Attention PPO  
* CNN over grid image â†’ patch tokens  
* Crossâ€‘attention with text token to spotlight goal region

### 3. BERT Fineâ€‘Tuning  
* **Sequenceâ€‘classification** clusters instructions by goal  
<p align="center">
  <img src="assets/images/classification.png" alt="BERT fine-tuning with text classification" width="40%">
</p>

* **Contrastive triplet loss** pushes apart nearâ€‘synonym commands  
  $\alpha_\text{dynamic}=Î»_1\lVert e_a-e_p\rVert^2 + Î»_2 d_\text{grid}^2$

---

## ExperimentsÂ &Â Results

| Setting                             | Stepsâ€¯â†’â€¯Opt. | Avg.Â Reward | Figure |
|-------------------------------------|-------------:|------------:|:------:|
| **No language (coords only)**       | 4â€¯k | **51** | <img src="assets/images/without_text.png" alt="Project banner" width="45%"/> |
| Raw BERT embeddings                 | â€” | <br>poor | <img src="assets/images/raw_bert.png" alt="Project banner" width="45%"/> |
| BERTÂ +Â ContrastiveÂ Learning         | 8â€¯k | ~50 | <img src="assets/images/contrastive_learning.png" alt="Project banner" width="45%"/> |
| BERTÂ +Â SequenceÂ Classification â˜…    | **4â€¯k** | **51** | <img src="assets/images/sequence_classification.png" alt="Project banner" width="45%"/> |
| Fineâ€‘tuned BERTÂ +Â GridÂ Attention    | 10â€¯k | 51 | <img src="assets/images/cross_attention.png" alt="Project banner" width="45%"/> |

_Key takeaway_: fineâ€‘tuned text embeddings using sequence classification let the agent hit optimal performance using only naturalâ€‘language commands.

---

## QuickÂ Start
To run **all experiments**, first setup the environment:

```bash
# 1â€¯â€“â€¯Install dependencies
conda create -n larla python=3.11
conda activate larla
pip install -r requirements.txt
```

---

## ReportÂ &Â Poster

For the full theoretical background, implementation details, and extended results, see:

| Resource | Link |
|----------|------|
| ğŸ“‘ **Project Report (PDF)** | [assets/report/report.pdf](assets/report/report.pdf) |
| ğŸ–¼ï¸ **Summary Poster** | [assets/poster/poster.png](assets/poster/poster.png) |

---


## References
1. **Maximeâ€¯Chevalierâ€‘Boisvert**, Dzmitryâ€¯Bahdanau, Salemâ€¯Lahlou, Lucasâ€¯Willems, Chitwanâ€¯Saharia, Thienâ€¯Huuâ€¯Nguyen, Yoshuaâ€¯Bengio.  
   *BabyAI: A Platform to Study the Sample Efficiency of Grounded Language Learning.*

2. **Liâ€¯Zhou**, **Kevinâ€¯Small**.  
   *Inverse Reinforcement Learning with Natural Language Goals.*
